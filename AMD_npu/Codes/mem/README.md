# ğŸ“ **README â€” AIE GEMM Microkernel (`mm.cc`)**

### *High-Performance Tiled Matrix Multiplication Kernel for AMD AIE*

---

## **ğŸ“Œ Overview**

`mm.cc` implements the **core compute microkernel** used inside the AIE array for performing high-performance matrix multiplication. It defines multiple **scalar** and **vectorized** GEMM kernels using the **AIE API (`aie::mmul`)**, supporting different data types and tiling configurations.

This file is the heart of the AIE compute stage â€” it determines:

* How each AIE tile computes its **32Ã—64Ã—64** (or other supported) sub-matrix
* How accumulator registers are reused
* How input tiles are loaded and streamed
* How results are written back into output tiles

These kernels are generated by MLIR-AIE and invoked by the host program (`test.cpp`).
They are optimized to achieve high compute utilization while fitting into AIE L1 memory constraints.

---

## **ğŸ“ Relationship to the Architecture**

From the project design document :

* The compute tile uses
  **A-tile = 32Ã—64**, **B-tile = 64Ã—64**, **C-tile = 32Ã—64**
* Tiles stream horizontally (B) and vertically (A)
* DMA uses **4 KBâ€“aligned** transfers
* The kernel must process these tiles efficiently and expose vectorized interfaces

`mm.cc` provides exactly this â€” a highly tuned kernel implementation matching the tile shapes and compute blocking described in the PDF.

---

## **âš™ï¸ Kernel Variants Implemented**

The file defines multiple GEMM microkernels:

### **1. Scalar Version**

Simple nested loops:

```cpp
matmul_scalar<T_in, T_out, rowA, colA, colB>()
```

Used for:

* Validation
* Debug builds
* When vectorized kernels are disabled

---

### **2. Vectorized 2Ã—2 Tile Expansion (`matmul_vectorized_2x2_mmul`)**

This is the default and most relevant kernel for your project.

It expands:

* **A in the M dimension Ã—2**
* **B in the N dimension Ã—2**

and computes a **2Ã—2 block of output tiles** inside a single iteration.
This improves:

* Register reuse
* Accumulator efficiency
* SIMD utilization

This corresponds to the **32Ã—64Ã—64 tile** used in your architecture design.

---

### **3. Vectorized 4Ã—2 and 4Ã—4 Expansions**

Used for:

* `i8` / mixed-precision inference
* `bf16` compute
* Wider N expansions

Examples include:

```
matmul_vectorized_4x2_mmul()
matmul_vectorized_4x4()
```

These kernels improve throughput when accumulator pressure is high (especially for `int8` and `bf16` workloads).

---

## **ğŸ§± Key Kernel Concepts**

### **âœ” Tiling and Shapes**

Each vectorized kernel is parameterized as:

```
r = tile rows per vector op
s = depth (K tile)
t = tile columns per vector op
```

For example:

* For **int16 GEMM**: `r=4, s=4, t=4`
* For **bf16**: `r=4, s=8, t=4`
* For **int8**: `r=4, s=8, t=8`

These shapes are chosen to match AIE hardware capabilities and SIMD widths (documented in the AIE API).

---

### **âœ” Accumulator Reuse**

Instead of recomputing C from scratch, the kernel:

1. Loads accumulated partial results from memory
2. Performs fused `mac` operations
3. Stores the updated tile back

This allows full K-dimension reductions.

---

### **âœ” Row-major / Column-major Handling**

The kernel supports both:

```
b_row_maj (input B)
c_row_maj (output C)
```

Matching your documented issue:
**Hardware stores tiles in column-major internally, but your reorder logic reconstructs row-major on host.**

---

### **âœ” Shape Validation**

Compile-time `static_assert` verifies:

* Dimensions are divisible by tile shapes
* Tensors are aligned correctly

This prevents illegal tile configurations.

---

## **ğŸ§© Exported C Functions (Extern â€œCâ€)**

The bottom section auto-generates C-callable functions such as:

```
matmul_i16_i16()
matmul_i16_i32()
matmul_bf16_f32()
matmul_i8_i32()
...
```

These are called by:

* MLIR-AIE runtime
* XRT kernel dispatch
* Your `test.cpp` host code

Every function expands to one of the vectorized templates based on datatype and tile size.

---

## **ğŸ“š Why This Kernel Matters for the Project**

From the architecture document:
You chose the **32Ã—64Ã—64 tile size** because it was the Pareto-optimal point considering:

* AIE local memory limits
* Vector register width
* Reuse of A/B tiles before eviction
* Streaming efficiency
* Supported shapes in `mm_32x64x64.o`

This microkernel implements exactly that logic.

It is the physical compute engine that makes your GEMM run at:

* **2.1â€“3.3 GFLOPS** on hardware
* **78Ã—â€“100Ã— faster than CPU** (benchmarked)

This file **is the core of the entire accelerator**.

---

## **ğŸ“¦ File Structure Summary**

```
mm.cc
â”‚
â”œâ”€â”€ matmul_scalar()                  # Reference scalar GEMM
â”œâ”€â”€ matmul_vectorized_2x2_mmul()     # Main kernel used in project
â”œâ”€â”€ matmul_vectorized_4x2_mmul()     # Optimized for int8
â”œâ”€â”€ matmul_vectorized_4x4()          # Optimized bf16/f32
â”‚
â”œâ”€â”€ Type-specific wrappers:
â”‚     matmul_i16_i16()
â”‚     matmul_i16_i32()
â”‚     matmul_bf16_f32()
â”‚     ...
â”‚
â””â”€â”€ Zero kernels and utility includes
```

---

## **ğŸ“ Notes for Users**

* `mm.cc` is **not meant to be modified frequently** â€” MLIR-AIE controls most of the integration.
* Ensure **DIM_M, DIM_K, DIM_N** are divisible by kernel tile sizes.
* Make sure row/column-major flags match your host-side reorder logic.
* For debugging, scalar kernels are helpful.
* For performance, always use vectorized kernels.

---

## **ğŸ”š Summary**

This microkernel implements the compute portion of your AIE-accelerated GEMM. Combined with the tile streaming hierarchy described in the design document , it forms the core of your high-performance vector retrieval pipeline.

It is highly optimized, datatype-aware, tile-aware, and designed to match the exact memory, alignment, and compute constraints of AMD AIE hardware.

---

If you want, I can also generate:

* **README for `test.cpp`** (similar style)
* **README for `array.py`**
* **README for `preprocessing/` folder**
* **A unified `/src/README.md`** referencing all components

Just say *â€œgive me the README for test.cppâ€*.
