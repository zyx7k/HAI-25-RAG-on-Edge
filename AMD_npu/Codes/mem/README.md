Below is the **full, clean, human-written, academic-style README.md (as Markdown code)** specifically for **mm.cc** ‚Äî NOT mixed with test.cpp, and NOT combining both.

This is the README you can paste directly into:

```
src/mm/README.md
```

or simply:

```
src/README_mm.md
```

It is written to match the tone and formatting of your main project README, while staying focused on **what mm.cc is**, **what design decisions it uses**, **how the vectorized kernels work**, and **how this relates to your document**.
It cites the document where needed (tile shape, design insights).
It avoids code-level noise and instead explains the ‚Äúwhy‚Äù and ‚Äúhow‚Äù.

---

# üìù **README ‚Äî AIE GEMM Microkernel (`mm.cc`)**

### *High-Performance Tiled Matrix Multiplication Kernel for AMD AIE*

---

## **üìå Overview**

`mm.cc` implements the **core compute microkernel** used inside the AIE array for performing high-performance matrix multiplication. It defines multiple **scalar** and **vectorized** GEMM kernels using the **AIE API (`aie::mmul`)**, supporting different data types and tiling configurations.

This file is the heart of the AIE compute stage ‚Äî it determines:

* How each AIE tile computes its **32√ó64√ó64** (or other supported) sub-matrix
* How accumulator registers are reused
* How input tiles are loaded and streamed
* How results are written back into output tiles

These kernels are generated by MLIR-AIE and invoked by the host program (`test.cpp`).
They are optimized to achieve high compute utilization while fitting into AIE L1 memory constraints.

---

## **üìê Relationship to the Architecture**

From the project design document :

* The compute tile uses
  **A-tile = 32√ó64**, **B-tile = 64√ó64**, **C-tile = 32√ó64**
* Tiles stream horizontally (B) and vertically (A)
* DMA uses **4 KB‚Äìaligned** transfers
* The kernel must process these tiles efficiently and expose vectorized interfaces

`mm.cc` provides exactly this ‚Äî a highly tuned kernel implementation matching the tile shapes and compute blocking described in the PDF.

---

## **‚öôÔ∏è Kernel Variants Implemented**

The file defines multiple GEMM microkernels:

### **1. Scalar Version**

Simple nested loops:

```cpp
matmul_scalar<T_in, T_out, rowA, colA, colB>()
```

Used for:

* Validation
* Debug builds
* When vectorized kernels are disabled

---

### **2. Vectorized 2√ó2 Tile Expansion (`matmul_vectorized_2x2_mmul`)**

This is the default and most relevant kernel for your project.

It expands:

* **A in the M dimension √ó2**
* **B in the N dimension √ó2**

and computes a **2√ó2 block of output tiles** inside a single iteration.
This improves:

* Register reuse
* Accumulator efficiency
* SIMD utilization

This corresponds to the **32√ó64√ó64 tile** used in your architecture design.

---

### **3. Vectorized 4√ó2 and 4√ó4 Expansions**

Used for:

* `i8` / mixed-precision inference
* `bf16` compute
* Wider N expansions

Examples include:

```
matmul_vectorized_4x2_mmul()
matmul_vectorized_4x4()
```

These kernels improve throughput when accumulator pressure is high (especially for `int8` and `bf16` workloads).

---

## **üß± Key Kernel Concepts**

### **‚úî Tiling and Shapes**

Each vectorized kernel is parameterized as:

```
r = tile rows per vector op
s = depth (K tile)
t = tile columns per vector op
```

For example:

* For **int16 GEMM**: `r=4, s=4, t=4`
* For **bf16**: `r=4, s=8, t=4`
* For **int8**: `r=4, s=8, t=8`

These shapes are chosen to match AIE hardware capabilities and SIMD widths (documented in the AIE API).

---

### **‚úî Accumulator Reuse**

Instead of recomputing C from scratch, the kernel:

1. Loads accumulated partial results from memory
2. Performs fused `mac` operations
3. Stores the updated tile back

This allows full K-dimension reductions.

---

### **‚úî Row-major / Column-major Handling**

The kernel supports both:

```
b_row_maj (input B)
c_row_maj (output C)
```

Matching your documented issue:
**Hardware stores tiles in column-major internally, but your reorder logic reconstructs row-major on host.**

---

### **‚úî Shape Validation**

Compile-time `static_assert` verifies:

* Dimensions are divisible by tile shapes
* Tensors are aligned correctly

This prevents illegal tile configurations.

---

## **üß© Exported C Functions (Extern ‚ÄúC‚Äù)**

The bottom section auto-generates C-callable functions such as:

```
matmul_i16_i16()
matmul_i16_i32()
matmul_bf16_f32()
matmul_i8_i32()
...
```

These are called by:

* MLIR-AIE runtime
* XRT kernel dispatch
* Your `test.cpp` host code

Every function expands to one of the vectorized templates based on datatype and tile size.

---

## **üìö Why This Kernel Matters for the Project**

From the architecture document:
You chose the **32√ó64√ó64 tile size** because it was the Pareto-optimal point considering:

* AIE local memory limits
* Vector register width
* Reuse of A/B tiles before eviction
* Streaming efficiency
* Supported shapes in `mm_32x64x64.o`

This microkernel implements exactly that logic.

It is the physical compute engine that makes your GEMM run at:

* **2.1‚Äì3.3 GFLOPS** on hardware
* **78√ó‚Äì100√ó faster than CPU** (benchmarked)

This file **is the core of the entire accelerator**.

---

## **üì¶ File Structure Summary**

```
mm.cc
‚îÇ
‚îú‚îÄ‚îÄ matmul_scalar()                  # Reference scalar GEMM
‚îú‚îÄ‚îÄ matmul_vectorized_2x2_mmul()     # Main kernel used in project
‚îú‚îÄ‚îÄ matmul_vectorized_4x2_mmul()     # Optimized for int8
‚îú‚îÄ‚îÄ matmul_vectorized_4x4()          # Optimized bf16/f32
‚îÇ
‚îú‚îÄ‚îÄ Type-specific wrappers:
‚îÇ     matmul_i16_i16()
‚îÇ     matmul_i16_i32()
‚îÇ     matmul_bf16_f32()
‚îÇ     ...
‚îÇ
‚îî‚îÄ‚îÄ Zero kernels and utility includes
```

---

## **üìù Notes for Users**

* `mm.cc` is **not meant to be modified frequently** ‚Äî MLIR-AIE controls most of the integration.
* Ensure **DIM_M, DIM_K, DIM_N** are divisible by kernel tile sizes.
* Make sure row/column-major flags match your host-side reorder logic.
* For debugging, scalar kernels are helpful.
* For performance, always use vectorized kernels.

---

## **üîö Summary**

This microkernel implements the compute portion of your AIE-accelerated GEMM. Combined with the tile streaming hierarchy described in the design document , it forms the core of your high-performance vector retrieval pipeline.

It is highly optimized, datatype-aware, tile-aware, and designed to match the exact memory, alignment, and compute constraints of AMD AIE hardware.

---

If you want, I can also generate:

* **README for `test.cpp`** (similar style)
* **README for `array.py`**
* **README for `preprocessing/` folder**
* **A unified `/src/README.md`** referencing all components

Just say *‚Äúgive me the README for test.cpp‚Äù*.
