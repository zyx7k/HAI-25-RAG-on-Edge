# **README — AIE GEMM Microkernel (`mm.cc`)**

### *High-Performance Tiled Matrix Multiplication Kernel for AMD AIE*

---

## **Overview**

`mm.cc` implements the **core compute microkernel** used inside the AIE array for performing high-performance matrix multiplication. It defines multiple **scalar** and **vectorized** GEMM kernels using the **AIE API (`aie::mmul`)**, supporting different data types and tiling configurations.

This file is the heart of the AIE compute stage — it determines:

* How each AIE tile computes its **32×64×64** (or other supported) sub-matrix
* How accumulator registers are reused
* How input tiles are loaded and streamed
* How results are written back into output tiles

These kernels are generated by MLIR-AIE and invoked by the host program (`test.cpp`).
They are optimized to achieve high compute utilization while fitting into AIE L1 memory constraints.

---

## **Relationship to the Architecture**

From the project design document :

* The compute tile uses
  **A-tile = 32×64**, **B-tile = 64×64**, **C-tile = 32×64**
* Tiles stream horizontally (B) and vertically (A)
* DMA uses **4 KB–aligned** transfers
* The kernel must process these tiles efficiently and expose vectorized interfaces

`mm.cc` provides exactly this — a highly tuned kernel implementation matching the tile shapes and compute blocking described in the PDF.

---

## **Kernel Variants Implemented**

The file defines multiple GEMM microkernels:

### **1. Scalar Version**

Simple nested loops:

```cpp
matmul_scalar<T_in, T_out, rowA, colA, colB>()
```

Used for:

* Validation
* Debug builds
* When vectorized kernels are disabled

---

### **2. Vectorized 2×2 Tile Expansion (`matmul_vectorized_2x2_mmul`)**

This is the default and most relevant kernel for your project.

It expands:

* **A in the M dimension ×2**
* **B in the N dimension ×2**

and computes a **2×2 block of output tiles** inside a single iteration.
This improves:

* Register reuse
* Accumulator efficiency
* SIMD utilization

This corresponds to the **32×64×64 tile** used in your architecture design.

---

### **3. Vectorized 4×2 and 4×4 Expansions**

Used for:

* `i8` / mixed-precision inference
* `bf16` compute
* Wider N expansions

Examples include:

```
matmul_vectorized_4x2_mmul()
matmul_vectorized_4x4()
```

These kernels improve throughput when accumulator pressure is high (especially for `int8` and `bf16` workloads).

---

## **Key Kernel Concepts**

### **✔ Tiling and Shapes**

Each vectorized kernel is parameterized as:

```
r = tile rows per vector op
s = depth (K tile)
t = tile columns per vector op
```

For example:

* For **int16 GEMM**: `r=4, s=4, t=4`
* For **bf16**: `r=4, s=8, t=4`
* For **int8**: `r=4, s=8, t=8`

These shapes are chosen to match AIE hardware capabilities and SIMD widths (documented in the AIE API).

---

### **✔ Accumulator Reuse**

Instead of recomputing C from scratch, the kernel:

1. Loads accumulated partial results from memory
2. Performs fused `mac` operations
3. Stores the updated tile back

This allows full K-dimension reductions.

---

### **Row-major / Column-major Handling**

The kernel supports both:

```
b_row_maj (input B)
c_row_maj (output C)
```

Matching your documented issue:
**Hardware stores tiles in column-major internally, but your reorder logic reconstructs row-major on host.**

---

### **✔ Shape Validation**

Compile-time `static_assert` verifies:

* Dimensions are divisible by tile shapes
* Tensors are aligned correctly

This prevents illegal tile configurations.

---

## **Exported C Functions (Extern “C”)**

The bottom section auto-generates C-callable functions such as:

```
matmul_i16_i16()
matmul_i16_i32()
matmul_bf16_f32()
matmul_i8_i32()
...
```

These are called by:

* MLIR-AIE runtime
* XRT kernel dispatch
* Your `test.cpp` host code

Every function expands to one of the vectorized templates based on datatype and tile size.

---

## **Why This Kernel Matters for the Project**

From the architecture document:
You chose the **32×64×64 tile size** because it was the Pareto-optimal point considering:

* AIE local memory limits
* Vector register width
* Reuse of A/B tiles before eviction
* Streaming efficiency
* Supported shapes in `mm_32x64x64.o`

This microkernel implements exactly that logic.

It is the physical compute engine that makes your GEMM run at:

* **2.1–3.3 GFLOPS** on hardware
* **78×–100× faster than CPU** (benchmarked)

This file **is the core of the entire accelerator**.

---

## **File Structure Summary**

```
mm.cc
│
├── matmul_scalar()                  # Reference scalar GEMM
├── matmul_vectorized_2x2_mmul()     # Main kernel used in project
├── matmul_vectorized_4x2_mmul()     # Optimized for int8
├── matmul_vectorized_4x4()          # Optimized bf16/f32
│
├── Type-specific wrappers:
│     matmul_i16_i16()
│     matmul_i16_i32()
│     matmul_bf16_f32()
│     ...
│
└── Zero kernels and utility includes
```

---
